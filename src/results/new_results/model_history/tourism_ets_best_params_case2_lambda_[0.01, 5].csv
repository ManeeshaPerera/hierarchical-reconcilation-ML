,0
batch_size,96.0
dropout_rate,0.10043807641435107
epochs,154.0
layers,4
learning_rate,0.00024783401831139553
max_norm_value,6.115880791833741
no_units_layer_5_1,90.0
no_units_layer_5_2,196.0
no_units_layer_5_3,23.0
no_units_layer_5_4,86.0
no_units_layer_5_5,113.0
reconciliation_loss_lambda,0.7375588398064568
no_layers,5
no_units_layer,"[90, 196, 23, 86, 113]"
