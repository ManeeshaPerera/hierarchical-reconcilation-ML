,0
batch_size,15.0
dropout_rate,0.3326417480170275
epochs,161.0
layers,2
learning_rate,0.0021623425732171567
max_norm_value,5.275089758249992
no_units_layer_3_1,65.0
no_units_layer_3_2,102.0
no_units_layer_3_3,181.0
reconciliation_loss_lambda,2.324032260515109
no_layers,3
no_units_layer,"[65, 102, 181]"
