,0
batch_size,29.0
dropout_rate,0.2912602230273682
epochs,167.0
layers,3
learning_rate,0.04487769372978452
max_norm_value,1.1863177576367505
no_units_layer_4_1,52.0
no_units_layer_4_2,168.0
no_units_layer_4_3,84.0
no_units_layer_4_4,194.0
reconciliation_loss_lambda,1.0758122312807434
no_layers,4
no_units_layer,"[52, 168, 84, 194]"
