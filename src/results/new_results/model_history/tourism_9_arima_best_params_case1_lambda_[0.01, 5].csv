,0
batch_size,26.0
dropout_rate,0.10320330681600587
epochs,140.0
layers,4
learning_rate,0.09024699950987639
max_norm_value,3.58326786463431
no_units_layer_5_1,109.0
no_units_layer_5_2,58.0
no_units_layer_5_3,224.0
no_units_layer_5_4,74.0
no_units_layer_5_5,78.0
reconciliation_loss_lambda,2.8574778788444517
no_layers,5
no_units_layer,"[109, 58, 224, 74, 78]"
