,0
batch_size,55.0
dropout_rate,0.12466683440484869
epochs,69.0
layers,4
learning_rate,0.06701383230133537
max_norm_value,2.3446011474498962
no_units_layer_5_1,220.0
no_units_layer_5_2,231.0
no_units_layer_5_3,112.0
no_units_layer_5_4,140.0
no_units_layer_5_5,124.0
reconciliation_loss_lambda,4.054253848668968
no_layers,5
no_units_layer,"[220, 231, 112, 140, 124]"
