,0
batch_size,22.0
dropout_rate,0.43740367747670317
epochs,123.0
layers,3
learning_rate,0.005469653731920085
max_norm_value,9.458779726866101
no_units_layer_4_1,222.0
no_units_layer_4_2,88.0
no_units_layer_4_3,6.0
no_units_layer_4_4,186.0
reconciliation_loss_lambda,1.2732522825243224
no_layers,4
no_units_layer,"[222, 88, 6, 186]"
