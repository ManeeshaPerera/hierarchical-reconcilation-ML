,0
batch_size,16.0
dropout_rate,0.22736985164595397
epochs,165.0
layers,4
learning_rate,0.060050242456451186
max_norm_value,5.086680132962682
no_units_layer_5_1,3.0
no_units_layer_5_2,69.0
no_units_layer_5_3,82.0
no_units_layer_5_4,199.0
no_units_layer_5_5,212.0
reconciliation_loss_lambda,2.0728239543161178
no_layers,5
no_units_layer,"[3, 69, 82, 199, 212]"
